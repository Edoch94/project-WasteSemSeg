{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edoardo/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0001, train_loss: 0.2193, val_loss: 0.0600, train_time 50.4556, val_time 24.1745\n",
      "epoch: 0002, train_loss: 0.1241, val_loss: 0.0477, train_time 50.6628, val_time 24.1689\n",
      "epoch: 0003, train_loss: 0.1069, val_loss: 0.0461, train_time 51.2700, val_time 25.7846\n",
      "epoch: 0004, train_loss: 0.1005, val_loss: 0.0484, train_time 52.5224, val_time 25.8401\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[39m.\u001b[39;49mmain()\n",
      "File \u001b[0;32m~/Documents/project-WasteSemSeg/train.py:80\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m epoch\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     79\u001b[0m _t[\u001b[39m'\u001b[39m\u001b[39mtrain time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtic()\n\u001b[0;32m---> 80\u001b[0m train_loss \u001b[39m=\u001b[39m fit(train_loader, net, criterion, optimizer)\n\u001b[1;32m     81\u001b[0m _t[\u001b[39m'\u001b[39m\u001b[39mtrain time\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtoc(average\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m \u001b[39m# print('training time of one epoch: {:.2f}s'.format(_t['train time'].diff))\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/project-WasteSemSeg/train.py:114\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(train_loader, net, criterion, optimizer)\u001b[0m\n\u001b[1;32m    111\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m    113\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m    115\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m    116\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/project-WasteSemSeg/model.py:247\u001b[0m, in \u001b[0;36mENet.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 247\u001b[0m     output, pooling_stack \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate:\n\u001b[1;32m    249\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(output, pooling_stack)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/project-WasteSemSeg/model.py:204\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m         pooling_stack\u001b[39m.\u001b[39mappend(pooling_indices)\n\u001b[1;32m    203\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m         output \u001b[39m=\u001b[39m layer(output)\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate:\n\u001b[1;32m    207\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mupsample(output, configs\u001b[39m.\u001b[39mconfig_Enet\u001b[39m.\u001b[39mcfg[\u001b[39m'\u001b[39m\u001b[39mTRAIN\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mIMG_SIZE\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/project-WasteSemSeg/model.py:147\u001b[0m, in \u001b[0;36mBottleNeck.forward\u001b[0;34m(self, input, pooling_indices)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     main \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m--> 147\u001b[0m other_net \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mSequential(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock1x1_1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmiddle_block, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock1x1_2)\n\u001b[1;32m    148\u001b[0m other \u001b[39m=\u001b[39m other_net(\u001b[39minput\u001b[39m)\n\u001b[1;32m    149\u001b[0m output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(main \u001b[39m+\u001b[39m other)\n",
      "File \u001b[0;32m~/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/modules/container.py:98\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n\u001b[1;32m     99\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], OrderedDict):\n\u001b[1;32m    100\u001b[0m         \u001b[39mfor\u001b[39;00m key, module \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/MLDL3/lib/python3.9/site-packages/torch/nn/modules/module.py:465\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_backward_hooks\u001b[39m\u001b[39m'\u001b[39m, OrderedDict())\n\u001b[1;32m    464\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_is_full_backward_hook\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 465\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m_forward_hooks\u001b[39;49m\u001b[39m'\u001b[39;49m, OrderedDict())\n\u001b[1;32m    466\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_forward_hooks_with_kwargs\u001b[39m\u001b[39m'\u001b[39m, OrderedDict())\n\u001b[1;32m    467\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m_forward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m, OrderedDict())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a486206e5124ebb0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a486206e5124ebb0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=\"runs/May18_00-20-34_edoch-ubuntu-xmg./exp/Enet_23-05-18_00-20-33_encoder_Enet_city_(224, 448)_lr_0.0005\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDL3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
